# 数据部分开发记录——03

此时项目的文件结构如下（省略部分文件，如__init__.py）：

```
QUANTITATIVE_INVESTMENT
├── doc
├── conf
├── logs
├── FinanceDataAPI  （数据系统子项目）
|   ├── python      （数据系统python源代码根目录）
|   |   ├── logger  （日志模块）
|   |   ├── manager （管理器模块）
|   |   ├── market_data （市场数据模块）
|   |   |   ├── trade_day   （交易日数据模块）
|   |   ├── pipeline    （数据处理流水线）
|   |   ├── utils   （工具模块）
|   |   ├── server_main.py  （数据系统程序入口点）
|   |   ├── settings.py   （数据系统常量配置）
|   ├── README.md
├── README.md
```

我们前面设计了任务作业流水线的架构模型，但是每次启动流水线都只能手动启动而且一次只能运行一次，现在我们需要再实现一个服务器程序，它要实时运行在服务器上，监听来自客户端的请求，并根据请求的内容，构建出要执行的任务，然后调用对应的流水线并执行，最后将结果返回给客户端。

要完成的需求：可以在每天的固定时间定时调用之前设计的获取交易日数据任务流水线完成交易日数据的更新与获取。

[//]: # (todo: )

---

## 前置知识引路

- python 异步编程
- WSGI
- ASGI
- python 服务器编程
-

---

## 服务器程序设计

我们采用python的fastapi框架来实现服务器程序。

以`server_main.py`为入口点，我们简单实现如下

```python
from fastapi import FastAPI
import uvicorn
from manager.component_manager import ComponentManager
from utils.log_utils import get_logger

logger = get_logger()
app = FastAPI()


@app.get("/")
async def root():
    return {"message": "Hello FinanceDataAPI"}


@app.get("/test")
async def test():
    # logger.debug("This is the test debug message.")
    # await asyncio.sleep(3)    # 异步阻塞不会停止整个程序，但会阻塞当前路由的响应
    # time.sleep(3)             # 非异步阻塞会停止整个程序，包括其他路由的响应
    return {"message": "Hello test"}


@app.on_event("startup")
async def startup_event():
    logger.info("Starting register component")

    component_manager = ComponentManager.get_instance()
    component_manager.register_component()


if __name__ == '__main__':
    uvicorn.run(app="server_main:app", host="0.0.0.0", port=8000, log_level="debug", reload=True)


```

我们在`@app.on_event("startup")`包装的函数`startup_event`
中可以实现一些初始化的操作（即会在服务器启动的时候调用一次），比如注册组件等功能，这里我们注册了组件，组件的注册是在`ComponentManager`
中实现的。

现在为了整个程序的完善，我们在开发服务器任务应用之前，先给我们的系统添加几个其他的管理器和模块。

### 存储引擎管理器

记得之前写在配置文件中的`storage_config_file_name`吗？这项配置指向的是存储配置文件的相对配置目录的路径，我们在`settings`
中读入后，可以实现一个`storage_engine_manager`来管理存储引擎的初始化和获取。

>
这样设计是为了对系统接入数据库的适配，市面上的数据库软件非常多（MySQL，MongoDB，Oracle，PostgreSQL等），我们可以通过配置文件来指定使用哪些数据库，然后在`storage_engine_manager`
中实现对应的数据库引擎的管理。
> 数据库引擎负责数据库的连接，断开，查询，写入等操作，也就是系统和数据库之前的中间件，在开发上来讲叫做数据库上下文对象，为系统提供对数据库的接口。

我们在`manager`目录下新建一个`storage_engine_manager.py`文件，实现如下：

```python
import json
import pymysql
import pymysql.cursors


class MySQLStorageEngine:
    def __init__(self, host, port, user, password, database, charset, cursorClass):
        self.host = host
        self.port = port
        self.user = user
        self.password = password
        self.database = database
        self.charset = charset
        self.cursorClass = cursorClass

    def __str__(self):
        return f"MySQLStorageEngine(host={self.host}, port={self.port}, user={self.user}, password={self.password}, " \
               f"database={self.database}, charset={self.charset}, cursorClass={self.cursorClass})"

    def __repr__(self):
        return self.__str__()

    def get_connection_context(self) -> pymysql.connections.Connection:
        conn = pymysql.connect(host=self.host, port=self.port, user=self.user, password=self.password,
                               database=self.database, charset=self.charset, cursorclass=self.cursorClass)

        return conn

    @classmethod
    def from_dict(cls, config_dict):
        host = config_dict.get("host")
        port = config_dict.get("port")
        user = config_dict.get("user")
        password = config_dict.get("password")
        database = config_dict.get("database")
        charset = config_dict.get("charset")
        if config_dict.get("cursorClass") == "DictCursor":
            cursorClass = pymysql.cursors.DictCursor
        else:
            raise ValueError(f"Unsupported cursor class: {config_dict.get('cursorClass')}")
        return cls(host, port, user, password, database, charset, cursorClass)


class StorageEngineManager:
    __instance = None

    def __new__(cls, *args, **kwargs):
        if StorageEngineManager.__instance is None:
            StorageEngineManager.__instance = object.__new__(cls)
        return StorageEngineManager.__instance

    @staticmethod
    def get_instance() -> 'StorageEngineManager':
        if StorageEngineManager.__instance is None:
            StorageEngineManager()
        return StorageEngineManager.__instance

    def __init__(self):
        self.storage_engine_type_list: list = []
        self.mysql_storage_engine_dict: dict[str, MySQLStorageEngine] = {}

    def read_storage_config(self, storage_config_file_path):
        try:
            with open(storage_config_file_path, "r") as f:
                storage_config_dict = json.load(f)
        except FileNotFoundError:
            print(f"Storage config file not found: {storage_config_file_path}")
            exit(1)
        except json.JSONDecodeError:
            print(f"Storage config file is not a valid JSON file: {storage_config_file_path}")
            exit(1)
        except Exception as e:
            print(f"Error occurred while reading storage config file: {storage_config_file_path}")
            print(e)
            exit(1)
        return storage_config_dict

    def load_storage_config(self, storage_config_file_path):
        storage_config_dict = self.read_storage_config(storage_config_file_path)
        self.storage_engine_type_list = list(storage_config_dict.keys())

        mysql_storage_config_dict = storage_config_dict.get("mysql")
        for database_name, database_storage_config in mysql_storage_config_dict.items():
            mysql_storage_config = MySQLStorageEngine.from_dict(database_storage_config)
            self.mysql_storage_engine_dict[database_name] = mysql_storage_config

    def get_mysql_storage_engine_dict(self) -> dict[str, MySQLStorageEngine]:
        return self.mysql_storage_engine_dict


if __name__ == '__main__':
    storager_manager = StorageEngineManager.get_instance()
    from settings import STORAGE_CONFIG_FILE_PATH

    storager_manager.load_storage_config(STORAGE_CONFIG_FILE_PATH)
    print(storager_manager)
    print(storager_manager.storage_engine_type_list)
    print(storager_manager.mysql_storage_engine_dict)
    ...

```

系统支持多个数据库的接入，也就是说可以进行多个数据库的同时操作。（分布式）

### 任务模型

任务模型的设计如下:

这里的任务模型其实就是一个键值对对象，我们规定任务请求传入的是json格式，监听程序要通过解析传入的json任务配置文件来构建任务。

规定一个任务对应一个`Job_item`对象，`Job_item`对象包含了任务的基本流程信息，流水线配置，对应的组件`ComponentItem`对象等。

```python
from pydantic import BaseModel
from typing import Optional, Any


class ComponentItem(BaseModel):
    component_class_name: str
    pre_component_name: Optional[str]
    component_arguments: dict[str, Any]
    ...


class JobItem(BaseModel):
    pipeline_structure: dict[str, ComponentItem]

```

再创建`Job`对象，结合前面的Item，可以创建一个`entity`包，用来存放实体类，将任务有关的写入`job.py`文件中。

```python
from pydantic import BaseModel
from typing import Optional, Any
from pipeline import Pipeline
from utils.log_utils import get_logger
from manager.component_manager import ComponentManager
import threading

logger = get_logger()


class ComponentItem(BaseModel):
    component_class_name: str
    pre_component_name: Optional[str]
    component_arguments: dict[str, Any]
    ...


class JobItem(BaseModel):
    pipeline_structure: dict[str, ComponentItem]


class Job:
    class JobStatus:
        WAITING = "waiting"
        RUNNING = "running"
        FINISHED = "finished"
        FAILED = "failed"

    def __init__(self):
        self.job_item: JobItem = None
        self.job_id: str = None
        self.pipeline: Pipeline = None
        self.mutex: threading.Lock = None
        self.status: bool = False
        ...

    @staticmethod
    def create_job(job_id: str, job_item: JobItem) -> "Job":
        job = Job()
        job.job_id = job_id
        job.job_item = job_item
        job.mutex = threading.Lock()

        job.pipeline = Pipeline()
        for component_name, component_item in job_item.pipeline_structure.items():
            component_class_name = component_item.component_class_name
            if component_item.pre_component_name:
                pre_component_item = job_item.pipeline_structure.get(component_item.pre_component_name)
                # todo

            component_arguments = component_item.component_arguments
            component_class = ComponentManager.get_instance().get_component_class_by_name(component_class_name)
            job.pipeline.add_component(component_class, component_arguments)
            logger.debug(f"Component added: {component_class_name} {component_arguments}")

        return job

    def run(self) -> bool:
        logger.info(f"{self} begin to run")
        self.pipeline.run()
        logger.info(f"{self} end to run")
        return True

    def __str__(self):
        return f"Job(job_id={self.job_id})"

    def __repr__(self):
        return self.__str__()

```

### 任务执行器

我们想将任务的管理和执行分开，即不把任务调用的代码写进`job_manager`中，而是另外实现一个`job_executor`
来执行任务，还可以加入线程池来实现多任务并发执行。

创建包`workers`，在其中创建`job_executor.py`文件，实现如下：

```python
import threading
from entity.job import Job
import queue
from utils.log_utils import get_logger, register_job_logger

logger = get_logger()


class JobExecutor(threading.Thread):
    def __init__(self, executor_id: int, job_queue: queue.Queue):
        super().__init__()
        self.executor_id = executor_id
        self.job_queue = job_queue
        self.current_job: Job = None
        self.start()

    def run_a_job(self, job: Job) -> bool:
        if not job:
            logger.error("Job is not set!")
            return False
        if job.status != job.JobStatus.WAITING:
            logger.error(f"{job} is {job.status}, not waiting!")
            return False

        register_job_logger(self.ident, job.job_id)
        logger.info(f"Thread id {self.ident} begin to run job: {job}")
        with job.mutex:
            job.status = job.JobStatus.RUNNING
            try:
                success = job.run()
            except Exception as e:
                logger.error(f"Job failed: {job}, error: {e}")
                success = False
                job.status = job.JobStatus.FAILED
                return success
            if success:
                logger.info(f"Job success finished: {job}")
                job.status = job.JobStatus.FINISHED
            else:
                logger.error(f"Job failed: {job}")
                job.status = job.JobStatus.FAILED
            logger.info(f"Thread id {self.ident} end to run job: {job}")
            return success

    def run(self) -> None:
        while True:
            job = self.job_queue.get(block=True)
            logger.info(f"JobExecutor {self.executor_id}:{self.ident} get a job: {job}")
            self.current_job = job

            self.run_a_job(job)

            self.current_job = None
            self.job_queue.task_done()


class JobExecuteMainThread(threading.Thread):
    _job_executor_instance_dict: dict[int:JobExecutor] = {}
    _job_queue = queue.Queue()
    _job_queue_mutex = threading.Lock()

    def __init__(self, max_executors: int = 4):
        super().__init__()
        self.max_executors = max_executors

    def initialize(self):
        logger.info(f"start job execute main thread with {self.max_executors} executors")

        for i in range(self.max_executors):
            job_executor = JobExecutor(i, self._job_queue)
            logger.info(f"JobExecutor {i}:{job_executor.ident} is created")
            self._job_executor_instance_dict[job_executor.ident] = job_executor

    def run(self) -> None:
        self.initialize()
        for job_executor_thread_ident, job_executor in self._job_executor_instance_dict.items():
            job_executor.join()

    @classmethod
    def add_job_to_execute(cls, job: Job):
        logger.debug(f"JobExecuteMainThread.add_job_to_execute: {job}")
        with cls._job_queue_mutex:
            cls._job_queue.put(job)
            logger.info(f"Job added to the queue: {job}")

    @classmethod
    def get_job_executor_by_thread_id(cls, thread_id):
        return cls._job_executor_instance_dict.get(thread_id, None)


if __name__ == '__main__':
    logger.info("JobExecutor test")
    job_execute_main_thread = JobExecuteMainThread()
    job_execute_main_thread.start()
    job_execute_main_thread.join()

```

### 任务管理器

`manager`包下实现`job_manager.py`

```python
import uuid
from datetime import datetime
from entity.job import JobItem, Job
from utils.log_utils import get_logger
from manager.component_manager import ComponentManager
from workers.job_executor import JobExecuteMainThread

logger = get_logger()


class JobManager:
    __instance = None

    def __new__(cls, *args, **kwargs):
        if JobManager.__instance is None:
            JobManager.__instance = super(JobManager, cls).__new__(cls)
        return JobManager.__instance

    @staticmethod
    def get_instance() -> 'JobManager':
        if JobManager.__instance is None:
            JobManager()
        return JobManager.__instance

    def __init__(self):
        self.job_id_to_job_dict: dict[str:Job] = {}  # todo: solve the problem of memory leak
        self.job_execute_main_thread = None

    def initialize(self, max_executors: int = 4):
        self.job_execute_main_thread: JobExecuteMainThread = JobExecuteMainThread(max_executors)
        self.job_execute_main_thread.start()

        logger.info(f"job manager initialized")

    def generate_job_id(self):
        timestamp = datetime.now()
        return f"{timestamp.strftime('%Y%m%d%H%M%S%f')}-{uuid.uuid4()}"

    def generate_job(self, job_item: JobItem):
        ComponentManager.get_instance().register_component()  # 每次加载job时都重新注册一次组件

        job_id = self.generate_job_id()
        logger.info(f"job_id: {job_id}")
        job = Job.create_job(job_id, job_item)
        logger.info(f"job created: {job}")
        job.status = Job.JobStatus.WAITING
        self.job_id_to_job_dict[job_id] = job

    def get_current_job_by_thread_id(self, thread_id):
        job_executor = JobExecuteMainThread.get_job_executor_by_thread_id(thread_id)
        if not job_executor:
            return None
        return job_executor.current_job

    def submit_job_to_execute(self, job_id):
        job: Job = self.job_id_to_job_dict.get(job_id, None)
        if job is None:
            logger.error(f"Job not found: {job_id}")
            return

        JobExecuteMainThread.add_job_to_execute(job)

    def get_job_status(self, job_id):
        job: Job = self.job_id_to_job_dict.get(job_id, None)
        if job is None:
            logger.error(f"Job not found: {job_id}")
            return None

        return job.status

    def run_all_jobs(self):
        current_job_dict = self.job_id_to_job_dict.copy()

        for job_id, job in current_job_dict.items():
            if job.status == Job.JobStatus.WAITING:
                self.submit_job_to_execute(job_id)


if __name__ == '__main__':
    job_manager = JobManager.get_instance()

    ...

```

### 定时任务管理器

记得前面在`settings.py`中我们还从配置文件中读取了`SCHEDULED_JOBS_CONFIG_DIR_PATH`吗。
这个代表的是定时任务配置文件的目录路径，默认是`examples/scheduler_jobs`。

为了实现定时任务，我们使用`apscheduler`
库，使用BackgroundScheduler来实现定时任务的调度，底层是多线程的实现（注意这里不用`apscheduler`
的异步调度器，不是事事都要用协程的，只用协程肯定无法实现并行，但是多线程可以并行，只是在调度上的开销会大一些，权衡好混合使用，追求效率的最大化）

我们在`manager`包下创建`scheduled_jobs_manager.py`文件，实现如下：

```python
import json
import os
import requests
from apscheduler.schedulers.background import BackgroundScheduler
from entity.scheduled_job import ScheduledJobItem
from utils.log_utils import get_logger
from settings import SERVER_HOST, SERVER_PORT

logger = get_logger()


class ScheduledJobsManager:
    __instance = None

    def __new__(cls, *args, **kwargs):
        if ScheduledJobsManager.__instance is None:
            ScheduledJobsManager.__instance = object.__new__(cls)
        return ScheduledJobsManager.__instance

    @staticmethod
    def get_instance() -> 'ScheduledJobsManager':
        if ScheduledJobsManager.__instance is None:
            ScheduledJobsManager()
        return ScheduledJobsManager.__instance

    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.job_item_list = []
        self.job_app_url = f"http://{SERVER_HOST}:{SERVER_PORT}/job/submit"

    def load_scheduled_jobs_config(self, scheduled_jobs_config_dir_path):
        logger.info(f"Loading scheduled jobs config: {scheduled_jobs_config_dir_path}")
        # 读取配置目录下的所有json文件
        if not os.path.exists(scheduled_jobs_config_dir_path):
            logger.error(f"Config directory not found: {scheduled_jobs_config_dir_path}")
            exit(1)

        for root, dirs, files in os.walk(scheduled_jobs_config_dir_path):
            for file in files:
                if file.endswith('.json'):
                    file_path = os.path.join(root, file)
                    self.load_scheduled_job_config(file_path)
        pass

    def load_scheduled_job_config(self, scheduled_job_config_path):
        logger.info(f"Loading scheduled job config: {scheduled_job_config_path}")

        with open(scheduled_job_config_path, "r") as f:
            scheduled_job_item = ScheduledJobItem.parse_raw(f.read())
            self.add_scheduled_job(scheduled_job_item)
        ...

    def add_scheduled_job(self, scheduled_job_item: ScheduledJobItem):
        self.job_item_list.append(scheduled_job_item)

        def submit_scheduled_job():
            logger.debug("submit scheduled job")
            # submit a job to the server
            job_json_dict = json.loads(scheduled_job_item.json())

            logger.debug(f"job_json: {job_json_dict}")
            logger.debug(f"type(job_json): {type(job_json_dict)}")

            # submit job
            response = requests.post(self.job_app_url, json=job_json_dict)

            logger.debug(f"response status code: {response.status_code}")
            logger.debug(f"response content: {response.content}")
            ...

        trigger_type = scheduled_job_item.trigger.type
        schedule = scheduled_job_item.trigger.arguments
        self.scheduler.add_job(submit_scheduled_job, trigger=trigger_type, **schedule)
        logger.debug(f"Scheduler job added: {scheduled_job_item}")

    def start_scheduler(self):
        self.scheduler.start()
        logger.info("Scheduler started.")


if __name__ == '__main__':
    from settings import SCHEDULED_JOBS_CONFIG_DIR_PATH

    scheduled_jobs_manager = ScheduledJobsManager.get_instance()
    scheduled_jobs_manager.load_scheduled_jobs_config(SCHEDULED_JOBS_CONFIG_DIR_PATH)
    ...

```

获取交易日数据的定时任务的示例如下：

```json
{
    "pipeline_structure": {
        "producer_0": {
            "component_class_name": "BaoStockTradeDayProducer",
            "pre_component_name": "",
            "component_arguments": {}
        },
        "processor_0": {
            "component_class_name": "BaoStockTradeDayProcessor",
            "pre_component_name": "producer_0",
            "component_arguments": {}
        },
        "storager_0": {
            "component_class_name": "BaoStockTradeDayStorager",
            "pre_component_name": "storager_0",
            "component_arguments": {}
        }
    },
    "trigger": {
        "type": "cron",
        "arguments": {
            "second": "0",
            "minute": "0",
            "hour": "9",
            "day": "*",
            "month": "*",
            "day_of_week": "*"
        }
    }
}
```

### 服务器程序

终于要开始写`server_main.py`了，让我们把系统运行起来！

```python
from fastapi import FastAPI
import uvicorn
from settings import STORAGE_CONFIG_FILE_PATH, SCHEDULED_JOBS_CONFIG_DIR_PATH, SERVER_HOST, SERVER_PORT, RELOAD_SYMBOL
from manager.component_manager import ComponentManager
from manager.storage_engine_manager import StorageEngineManager
from manager.scheduled_jobs_manager import ScheduledJobsManager
from apps.job_app import job_router
from utils.log_utils import get_logger

logger = get_logger()
app = FastAPI()


@app.get("/")
async def root():
    return {"message": "Hello FinanceDataAPI"}


@app.get("/test")
async def test():
    # logger.debug("This is the test debug message.")
    # await asyncio.sleep(3)    # 异步阻塞不会停止整个程序，但会阻塞当前路由的响应
    # time.sleep(3)             # 非异步阻塞会停止整个程序，包括其他路由的响应
    return {"message": "Hello test"}


@app.on_event("startup")
async def startup_event():
    logger.info("Starting register component")

    component_manager = ComponentManager.get_instance()
    component_manager.register_component()

    logger.info("Starting load storage config")

    storager_manager = StorageEngineManager.get_instance()
    storager_manager.load_storage_config(STORAGE_CONFIG_FILE_PATH)

    logger.info("Starting load scheduled jobs config")
    scheduled_jobs_manager = ScheduledJobsManager.get_instance()
    scheduled_jobs_manager.load_scheduled_jobs_config(SCHEDULED_JOBS_CONFIG_DIR_PATH)

    logger.info("Starting BackgroundScheduler")
    scheduled_jobs_manager.start_scheduler()

    app.include_router(job_router)


if __name__ == '__main__':
    uvicorn.run(app="server_main:app", host=SERVER_HOST, port=SERVER_PORT, reload=RELOAD_SYMBOL)
```

## 遇到的问题和解决

>
上面的程序可以正常运行，这个文档内部的代码是便开发边记录的，可能和实际项目中的程序代码不一样，所以如果是单纯按照文档中代码来运行的话可能会出现程序性的报错和无法执行，具体的debug过程不是我们文档记录的主要内容，在运行过程中遇到类似的问题需要自行解决，我们主要记录开发上的设计思想和大致的代码实现。

即使代码可以正常运行，还是难以避免逻辑上存在的问题，以下列举开发过程中遇到的几个问题

### 1. 日志记录混乱

我们在最开始写日志系统的时候只初始化了一个logger，但是由于任务流水线的加入，把所有的日志都记录在同一个文件里会非常不利于我们的日志管理和调试，所以日志系统应该重新设计，为每个具体的任务提供单独的日志对象，并且对应的日志文件也应该分等级记录到不同任务的日志文件夹中。

首先删除了原来的manual logger 和 default logger ，新建了一个logger factory

```python
import os
import logging
import threading
from settings import LOG_DIR_PATH, log_config_dict


class LoggerFactory(object):
    DEFAULT_LOG_NAME = "FinanceDataAPI_default_logger"
    DEFAULT_LOG_FORMAT_STR = log_config_dict.get("default_log_format_str",
                                                 "[%(levelname)s] [%(asctime)s] [job_id] [%(process)s:%(thread)s] - [%(module)s.%(funcName)s] [line:%(lineno)d]: %(message)s")
    DEFAULT_LEVEL = log_config_dict.get("default_level", logging.INFO)
    DEFAULT_ENCODING = log_config_dict.get("default_encoding", "utf-8")
    DEFAULT_LOG_DIR_PATH = LOG_DIR_PATH
    DEFAULT_LOG_PATH = os.path.join(LOG_DIR_PATH, "finance_data_api.log")
    default_logger = None
    log_format_dict = {
        "default": DEFAULT_LOG_FORMAT_STR,
    }

    thread_id_to_logger_dict = {}

    class LoggerType:
        DEFAULT = "default"
        SERVER = "server"
        JOB = "job"

    @classmethod
    def get_logger(cls, logger_type=LoggerType.DEFAULT, log_level=DEFAULT_LEVEL):

        if logger_type == cls.LoggerType.DEFAULT:
            return cls.get_default_logger(log_level=log_level)
        elif logger_type == cls.LoggerType.SERVER:
            return cls.get_job_logger("server", log_level=log_level)
        elif logger_type == cls.LoggerType.JOB:
            return cls.get_job_logger(None, log_level=log_level)

    @classmethod
    def get_log_level_name(cls, log_level):
        return logging.getLevelName(log_level)

    @classmethod
    def create_job_logger(cls, thread_id, job_id, log_level=DEFAULT_LEVEL):
        logger = cls.create_logger(log_level=log_level, log_type=cls.LoggerType.JOB, job_id=job_id)
        cls.thread_id_to_logger_dict[thread_id] = logger
        logger.info(f"job logger created for job_id: {job_id}, thread_id: {thread_id}")
        ...

    @classmethod
    def get_job_logger(cls, job_id, log_level=DEFAULT_LEVEL):

        thread_id = threading.get_ident()
        job_logger = cls.thread_id_to_logger_dict.get(thread_id, None)
        if not job_logger:
            if job_id is None:
                raise Exception("job logger has not been created yet")
            cls.create_job_logger(threading.get_ident(), job_id, log_level=log_level)

        job_logger = cls.thread_id_to_logger_dict.get(thread_id, None)
        if not job_logger:
            raise Exception("Job logger not found")
        return job_logger

    @classmethod
    def create_default_logger(cls, log_level=DEFAULT_LEVEL):
        default_logger = logging.getLogger(cls.DEFAULT_LOG_NAME)

        formatter = logging.Formatter(cls.replace_format_str(cls.DEFAULT_LOG_FORMAT_STR))
        to_console = logging.StreamHandler()
        to_console.setFormatter(formatter)
        default_logger.addHandler(to_console)

        os.makedirs(cls.DEFAULT_LOG_DIR_PATH, exist_ok=True)
        file_handler = logging.FileHandler(cls.DEFAULT_LOG_PATH, encoding=cls.DEFAULT_ENCODING)
        file_handler.setFormatter(formatter)
        default_logger.addHandler(file_handler)

        if log_level:
            default_logger.setLevel(log_level)
        return default_logger

    @classmethod
    def get_default_logger(cls, log_level=DEFAULT_LEVEL):
        if cls.default_logger:
            return cls.default_logger

        default_logger = cls.create_default_logger(log_level=log_level)
        return default_logger

    @classmethod
    def replace_format_str(cls, format_str: str, *args, **kwargs):
        try:
            replaced_formatter_str = format_str.replace("job_id", kwargs.get("job_id", "FDA_Server"))
            return replaced_formatter_str
        except KeyError:
            pass

    @classmethod
    def get_log_dir_path(cls, logger_name: str = None, log_level=DEFAULT_LEVEL, log_type: str = LoggerType.SERVER,
                         *args,
                         **kwargs) -> str:

        log_dir_path = ""
        if log_type == cls.LoggerType.SERVER:
            log_dir_path = os.path.join(cls.DEFAULT_LOG_DIR_PATH, "server")
        elif log_type == cls.LoggerType.JOB:
            job_id = kwargs.get("job_id", None)
            if not job_id:
                raise ValueError("job_id is required for job log")
            log_dir_path = os.path.join(cls.DEFAULT_LOG_DIR_PATH, job_id)

        os.makedirs(log_dir_path, exist_ok=True)
        return log_dir_path

    @classmethod
    def create_log_file_handler(cls, log_dir_path, log_level=DEFAULT_LEVEL,
                                *args,
                                **kwargs):
        formatter = logging.Formatter(cls.replace_format_str(cls.DEFAULT_LOG_FORMAT_STR, *args, **kwargs))

        log_file_path = os.path.join(log_dir_path, f"{cls.get_log_level_name(log_level)}.log")
        file_handler = logging.FileHandler(log_file_path)
        file_handler.setLevel(log_level)
        file_handler.setFormatter(formatter)
        return file_handler

    @classmethod
    def create_stream_handler(cls, log_level=DEFAULT_LEVEL, *args, **kwargs):
        formatter = logging.Formatter(cls.replace_format_str(cls.DEFAULT_LOG_FORMAT_STR, *args, **kwargs))
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(log_level)
        stream_handler.setFormatter(formatter)
        return stream_handler

    @classmethod
    def create_logger(cls, logger_name: str = None, log_level=DEFAULT_LEVEL, log_type: str = LoggerType.SERVER, *args,
                      **kwargs):

        log_dir_path = cls.get_log_dir_path(logger_name=logger_name, log_level=log_level, log_type=log_type, *args,
                                            **kwargs)
        logger = logging.getLogger(logger_name)
        # 创建多个handler
        to_console = cls.create_stream_handler(log_level=log_level, *args, **kwargs)
        error_handler = cls.create_log_file_handler(log_dir_path, log_level=logging.ERROR, *args, **kwargs)
        info_handler = cls.create_log_file_handler(log_dir_path, log_level=logging.INFO, *args, **kwargs)
        debug_handler = cls.create_log_file_handler(log_dir_path, log_level=logging.DEBUG, *args, **kwargs)

        logger.addHandler(to_console)
        logger.addHandler(error_handler)
        logger.addHandler(info_handler)
        logger.addHandler(debug_handler)

        logger.setLevel(log_level)
        return logger


def get_logger(log_level=logging.INFO):
    current_thread_id = threading.get_ident()
    # print(f"current_thread_id: {current_thread_id}")
    if threading.current_thread() == threading.main_thread():
        logger_type = LoggerFactory.LoggerType.SERVER
    else:
        logger_type = LoggerFactory.LoggerType.JOB
    logger = LoggerFactory.get_logger(logger_type=logger_type, log_level=log_level)
    if not logger:
        logger = LoggerFactory.get_default_logger(log_level=log_level)
    return logger


if __name__ == '__main__':
    from settings import LOG_DIR_PATH

```

> 对应的常量变化是因为有修改配置文件

然后修改`utils`包下的`log_utils.py`文件

```python
from logger.logger_factory import LoggerFactory, get_logger as _get_logger
import logging


def register_job_logger(thread_id, job_id, log_level=LoggerFactory.DEFAULT_LEVEL):
    LoggerFactory.create_job_logger(thread_id, job_id, log_level=log_level)


def get_logger(log_level=logging.INFO):
    return _get_logger(log_level=log_level)


if __name__ == '__main__':
    logger = get_logger(logging.DEBUG)
    logger.debug("Debug message")
    logger.info("Info message")
    logger.warning("Warning message")
    logger.error("Error message")

```

这样就可在全局的任何地方直接通过以下代码获取日志对象，而且不用关系传入的参数，我们实现的方法可以自动根据当前线程的类型来创建与选择日志对象，非常的简洁，方便，其他部分的开发不必再去关系日志对象的创建和管理。

```python
from utils.log_utils import get_logger

logger = get_logger()
```


### 2. 动态导入模块——组件热更新功能失效

我们采用importlib和inspect模块来实现组件热更新，但是在实际运行中发现热更新过程中重新导入的模块并没有更新。


| 存在问题的原因 | 解决方案 |
| ---| |
|直接使用`importlib.import_module`在模块不存在sys.module的时候会进行导入，也就是如果调用两次`importlib.import_module`并不会重新导入模块，就像缓存机制一样|使用`importlib.reload`函数或者判断模块是否存在sys.modules中，如果存在则使用`del sys.modules[module_name]`删除模块再重新导入|
|即使是reload了模块，但是模块中通过`from xxx import `的其他模块或者对象并不会被重新reload，因为我们的系统是组件注册文件和组件文件分开的，就会造成只重新导入了注册文件，但是实际上的组件类对象并没有重载|在重载了组件注册文件后，继续往深一层去寻找组件类对象，通过对象获取对象的组件文件所在的模块后进行重新导入|

> 第一个问题是python import的缓存机制问题，第二个问题就好像你去公安局申请改名字，假设公安局要通过你的户口本来获取你的身份证，但是只做了第一层户口本上的改名，而没有在你的身份证上改名，这样你实际上的身份证还是原来的名字，并没有改名成功。


### 3. 存储时间过长

因为获取的数据量可能非常多，而在`pymysql`中是通过提交事务来实现对数据库的操作的，一般的逐行插入数据的方式会非常慢，类似代码如下

```python
    def storage_trade_day_data(self, processed_trade_day_data: pd.DataFrame) -> pd.DataFrame:
        sql = "CALL insert_trade_day(%s, %s)"

        logger.debug(f"processed_trade_day_data.shape: {processed_trade_day_data.shape}")
        for mysql_storage_engine_name, mysql_storage_engine in storage_engine_manager.get_mysql_storage_engine_dict().items():
            logger.debug(f"mysql_storage_engine_name: {mysql_storage_engine_name}")
            logger.debug(f"mysql_storage_engine: {mysql_storage_engine}")
            conn = mysql_storage_engine.get_connection_context()
            with conn:
                with conn.cursor() as cursor:
                    logger.debug(f"execute begin")
                    for index, row in processed_trade_day_data.iterrows():
                        cursor.execute(sql, (*row,))
                    logger.debug(f"execute end")
                logger.debug(f"commit begin")
                conn.commit()
                logger.debug(f"commit end")

        success = pd.DataFrame([['success']], columns=['status'])
        return success

```


```
[DEBUG] [2024-04-26 17:07:09,957] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:44]: processed_trade_day_data.shape: (981, 2)
[DEBUG] [2024-04-26 17:07:09,957] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:46]: mysql_storage_engine_name: XXX
[DEBUG] [2024-04-26 17:07:09,957] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:47]: mysql_storage_engine: MySQLStorageEngine(host=127.0.0.1, port=3307, user=root, password=********, database=market_data, charset=utf8mb4, cursorClass=<class 'pymysql.cursors.DictCursor'>)
[DEBUG] [2024-04-26 17:07:10,991] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:51]: execute begin
[DEBUG] [2024-04-26 17:07:11,673] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:54]: execute end
[DEBUG] [2024-04-26 17:07:11,673] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:55]: commit begin
[DEBUG] [2024-04-26 17:07:11,675] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:57]: commit end
[DEBUG] [2024-04-26 17:07:11,675] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:46]: mysql_storage_engine_name: XXX
[DEBUG] [2024-04-26 17:07:11,675] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:47]: mysql_storage_engine: MySQLStorageEngine(host=X.X.X.X, port=3307, user=root, password=********, database=market_data, charset=utf8mb4, cursorClass=<class 'pymysql.cursors.DictCursor'>)
[DEBUG] [2024-04-26 17:07:11,759] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:51]: execute begin
[DEBUG] [2024-04-26 17:07:24,829] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:54]: execute end
[DEBUG] [2024-04-26 17:07:24,829] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:55]: commit begin
[DEBUG] [2024-04-26 17:07:24,841] [20240426162328150856-dc1597a8-8bf1-4320-a394-fdb8cf27200e] [10812:7976] - [storager.storage_trade_day_data] [line:57]: commit end
```

可以看出主要时间都在`execute`的过程，起初我还以为`execute`只是将sql字符串拼接起来，最后`commit`才会提交，但好像并不是这样的，这跟`pymysql`有关，经过查阅，发现应该底层是MySQL事务相关的操作，所以`execute`执行的时候就是执行了，会产生与数据库的交互，数据量一多，`execute`就会很慢，这里足足981条数据足足花了13秒（总共两个数据库，一个是本地的，所以会快一点，另一个是测试的云服务器，符合生产环境，所以我们关注下面那个的耗时）

#### 解决方案1

经过查阅，发现`pymysql`还支持`executemany`，看别人的使用博客和介绍都说这个方法适合大量数据，可以显著缩短时间

```python
    def storage_trade_day_data(self, processed_trade_day_data: pd.DataFrame) -> pd.DataFrame:
        sql = "CALL insert_trade_day(%s, %s)"

        logger.debug(f"processed_trade_day_data.shape: {processed_trade_day_data.shape}")
        for mysql_storage_engine_name, mysql_storage_engine in storage_engine_manager.get_mysql_storage_engine_dict().items():
            logger.debug(f"mysql_storage_engine_name: {mysql_storage_engine_name}")
            logger.debug(f"mysql_storage_engine: {mysql_storage_engine}")
            conn = mysql_storage_engine.get_connection_context()
            with conn:
                with conn.cursor() as cursor:
                    args = []
                    for index, row in processed_trade_day_data.iterrows():
                        args.append((*row,))
                    logger.debug(f"execute begin")
                    cursor.executemany(sql, args)
                    logger.debug(f"execute end")
                logger.debug(f"commit begin")
                conn.commit()
                logger.debug(f"commit end")

        success = pd.DataFrame([['success']], columns=['status'])
        return success
```

```
[DEBUG] [2024-04-26 17:36:58,846] [20240426173653778564-0abc90fd-9f50-4f5c-a820-7055b3ea1608] [14164:35204] - [storager.storage_trade_day_data] [line:54]: execute begin
[DEBUG] [2024-04-26 17:37:15,317] [20240426173653778564-0abc90fd-9f50-4f5c-a820-7055b3ea1608] [14164:35204] - [storager.storage_trade_day_data] [line:56]: execute end
[DEBUG] [2024-04-26 17:37:15,318] [20240426173653778564-0abc90fd-9f50-4f5c-a820-7055b3ea1608] [14164:35204] - [storager.storage_trade_day_data] [line:57]: commit begin
[DEBUG] [2024-04-26 17:37:15,334] [20240426173653778564-0abc90fd-9f50-4f5c-a820-7055b3ea1608] [14164:35204] - [storager.storage_trade_day_data] [line:59]: commit end
```

但是结果并没有好到哪里去，时间甚至变长了，花了17秒

#### 解决方案2

解决方案1之所以给人一种确实会变快的直觉，是因为它符合我们一开始的想法，就是不单独执行每条sql语句，而是将sql语句拼接起来执行，这样交互次数就从n降为1了，但是结果很明显`executemany`并没有达到我们的预期，那我们这里自己实现。

```python
    def storage_trade_day_data(self, processed_trade_day_data: pd.DataFrame) -> pd.DataFrame:
        singal_sql = "CALL insert_trade_day('{}', '{}');"
        final_sql = ""

        logger.debug(f"processed_trade_day_data.shape: {processed_trade_day_data.shape}")
        for mysql_storage_engine_name, mysql_storage_engine in storage_engine_manager.get_mysql_storage_engine_dict().items():
            logger.debug(f"mysql_storage_engine_name: {mysql_storage_engine_name}")
            logger.debug(f"mysql_storage_engine: {mysql_storage_engine}")
            conn = mysql_storage_engine.get_connection_context()
            with conn:
                with conn.cursor() as cursor:
                    for index, row in processed_trade_day_data.iterrows():
                        logger.debug(f"row: {row}")
                        sql = singal_sql.format(*row)
                        final_sql += sql + "\n"
                    logger.debug(f"before execute final_sql: {final_sql}")
                    cursor.execute(final_sql)
                    logger.debug(f"after execute")
                conn.commit()
        logger.debug(f"finish storage!")

        success = pd.DataFrame([['success']], columns=['status'])
        return success

```

因为MySQL 8 版本的原因，默认不允许一次性执行多条语句，需要在获取连接时添加一个参数`CLIENT.MULTI_STATEMENTS`

```python
    def get_connection_context(self) -> pymysql.connections.Connection:
        conn = pymysql.connect(host=self.host, port=self.port, user=self.user, password=self.password,
                               database=self.database, charset=self.charset, cursorclass=self.cursorClass,
                               client_flag=CLIENT.MULTI_STATEMENTS)
```

```
[DEBUG] [2024-04-26 17:42:59,161] [20240426174251282264-e81a3f9d-01d5-4d0e-93e8-0a7086ace91c] [30772:20680] - [storager.storage_trade_day_data] [line:32]: before execute final_sql ...
[DEBUG] [2024-04-26 17:42:59,166] [20240426174251282264-e81a3f9d-01d5-4d0e-93e8-0a7086ace91c] [30772:20680] - [storager.storage_trade_day_data] [line:34]: after execute

```

将sql拼接起来后一次性执行，这样就只有一次交互，时间大大缩短，从需要十几秒到只用了不到1秒就执行完毕，性能提升是数量级的。


### MYSQL 查询除了系统库以外的所有库所有表占用空间大小

```mysql
SELECT table_schema                                           AS `Database`,
       table_name                                             AS `Table`,
       round(((data_length + index_length) / 1024 / 1024), 2) AS `Size (MB)`
FROM information_schema.tables
WHERE table_schema NOT IN ('information_schema', 'mysql', 'performance_schema', 'sys')
ORDER BY (data_length + index_length) DESC
LIMIT 10;
```